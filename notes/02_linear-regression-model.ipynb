{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9648d9-7f81-41d5-b4fe-96eb01ee178c",
   "metadata": {},
   "source": [
    "# Linear Regresstion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcec9f7-295d-4619-905a-b74276d8954b",
   "metadata": {},
   "source": [
    "Simple Linear Regression model\n",
    "\n",
    "$\n",
    "y = a + bx\n",
    "$\n",
    "\n",
    "we expect the following values to be predicted via training as close as possible: \n",
    "\n",
    "$\n",
    "a = 1\\\\\n",
    "b = 2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48002dfb-0c60-4200-b077-c8bf5e437ffd",
   "metadata": {},
   "source": [
    "## Warm Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f3392f-f579-4be7-8a85-c170c014e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# mean = average\n",
    "mean_val = np.array([1, 3, 5, 11]).mean()\n",
    "print(mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e17234-17c1-4b0f-a929-d1ebe017864b",
   "metadata": {},
   "source": [
    "## Sample Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec463d2a-6a7e-43d5-8c8d-451bbe099df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate sample data\n",
    "np.random.seed(42)\n",
    "a_answer = 1\n",
    "b_answer = 2\n",
    "x = np.random.rand(100, 1)\n",
    "y = a_answer + b_answer * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# uses the remaining indices for validation\n",
    "eval_idx = idx[80:]\n",
    "\n",
    "# generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_eval, y_eval = x[eval_idx], y[eval_idx]\n",
    "\n",
    "plt.plot(x_train, y_train, 'x', color='r', label='Training data')\n",
    "plt.plot(x_eval, y_eval, 'o', color='b', label='Validation data')\n",
    "plt.xlabel(r'x value')\n",
    "plt.ylabel(r'y value')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0a622-0d73-483f-a787-d0082f27f53b",
   "metadata": {},
   "source": [
    "## Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a845758-0b38-44b2-84fc-06413c39edb1",
   "metadata": {},
   "source": [
    "**Mean Square Error (MSE)** means **loss**\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - (a + bx_i))^2\n",
    "$$\n",
    "\n",
    "Gradients Calculation\n",
    "\n",
    "$$\n",
    "a\\_grad = -2 \\times \\sum_{i=1}^{N}\\big(y_i - (a + bx_i)\\big)\\\\\n",
    "b\\_grad = -2 \\times \\sum_{i=1}^{N}\\big(y_i - (a + bx_i)) \\times (x_i) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d85e1-bf75-41dd-810d-4928fd71b92e",
   "metadata": {},
   "source": [
    "## Numpy Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728e79ea-7f0e-4868-ac85-71ab759d8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354094] [1.96896411]\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "# Initializes parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Computes our model's predicted output\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # Computes gradients for both \"a\" and \"b\" parameters\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9eafb-b38e-4b8f-b22a-2ec54d607be0",
   "metadata": {},
   "source": [
    "## Scikit-learn Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdaa3c88-b9b2-4b8b-8faa-cbbee1bd7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02354075] [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec0b2d-b5f2-4282-9afb-700ab5e2043d",
   "metadata": {},
   "source": [
    "## PyTorch Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e120a51-e8e0-4171-819b-3e4f10aea734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606631f7-f10b-4a50-a6fc-ba1ec43e735a",
   "metadata": {},
   "source": [
    "### Manual Parameters Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0e8467-516b-49ee-8c14-e91bd1fae657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "# Initializes parameters \"a\" and \"b\" randomly\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more manual computation of gradients using PyTorch! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    # Let's check the computed gradients...\n",
    "    # print(a.grad)\n",
    "    # print(b.grad)\n",
    "    \n",
    "    # What about UPDATING the parameters?\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    # In PyTorch, every method that ends with an underscore (_) makes changes in-place,\n",
    "    # meaning, they will modify the underlying variable.\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a1530-63d4-425b-b65d-a3b50dbcd782",
   "metadata": {},
   "source": [
    "### Using PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32e66c5-7c7c-4419-b5f7-24f6111d882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = torch.optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    loss.backward()    \n",
    "    \n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e56681-744b-4a2c-b8f6-87c653bcc1cb",
   "metadata": {},
   "source": [
    "### Using PyTorch Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e234e472-7846-48c2-b230-21fb7fe0a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # No more manual loss!\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "    # Computes gradients\n",
    "    loss.backward()\n",
    "    # Updates parameters and zeroes gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abee09a-bc60-4a96-83d0-fa0dcef2b450",
   "metadata": {},
   "source": [
    "### Using Model Approach\n",
    "\n",
    "Using Model, we can organize code more in modular way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5108528-7f0d-4ce5-86e7-07b9c6ceae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "class ManualLinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = torch.nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # train() does NOT perform a training step.\n",
    "    # Its only purpose is to set the model to training mode.\n",
    "    # Some models use mechanisms like Dropout which have distinct behaviors in training and evaluation phases.\n",
    "    model.train()\n",
    "\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "\n",
    "    # Computes loss\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    # Computes gradients\n",
    "    loss.backward()\n",
    "    # Updates parameters and zeroes gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecdbe6-9853-4ab9-aa3c-93ed8d6aabf9",
   "metadata": {},
   "source": [
    "### Using PyTorch Build-in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4284ee7-624b-46c1-a6e6-2c8be1f9948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[0.7645]])), ('0.bias', tensor([0.8300]))])\n",
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = torch.nn.Sequential(torch.nn.Linear(1,1)).to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # train() does NOT perform a training step.\n",
    "    # Its only purpose is to set the model to training mode.\n",
    "    # Some models use mechanisms like Dropout which have distinct behaviors in training and evaluation phases.\n",
    "    model.train()\n",
    "\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "\n",
    "    # Computes loss\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    # Computes gradients\n",
    "    loss.backward()\n",
    "    # Updates parameters and zeroes gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73774a1-7f99-4a59-8a24-ee10cc2e110f",
   "metadata": {},
   "source": [
    "### Using and PyTorch TensorDataset Dataloader\n",
    "\n",
    "Until now, we have used the whole training data at every training step. It has been batch gradient descent all along. This is fine for our ridiculously small dataset, but if we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ddb77f-19fd-4348-8d1a-5aeb152cdb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n",
      "OrderedDict([('0.weight', tensor([[-0.2343]])), ('0.bias', tensor([0.9186]))])\n",
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Data Setup\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "# mini-bach config\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(1,1)).to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # train() does NOT perform a training step.\n",
    "        # Its only purpose is to set the model to training mode.\n",
    "        # Some models use mechanisms like Dropout which have distinct behaviors in training and evaluation phases.\n",
    "        model.train()\n",
    "\n",
    "        # No more manual prediction!\n",
    "        # yhat = a + b * x_tensor\n",
    "        yhat = model(x_train_tensor)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y_train_tensor, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # record loss\n",
    "        losses.append(loss)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b976dc7-0fe3-4a3e-b348-bd0c0d19b572",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d585d99-7fa6-4a03-b0b0-74ec1e10d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n",
      "OrderedDict([('0.weight', tensor([[0.7827]])), ('0.bias', tensor([0.4601]))])\n",
      "OrderedDict([('0.weight', tensor([[1.9690]])), ('0.bias', tensor([1.0235]))])\n"
     ]
    }
   ],
   "source": [
    "# Data Setup\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "x_val_tensor = torch.from_numpy(x_eval).float()\n",
    "y_val_tensor = torch.from_numpy(y_eval).float()\n",
    "val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "print(train_data[0])\n",
    "# mini-bach config\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16, shuffle=True)\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(1,1)).to(device)\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # train() does NOT perform a training step.\n",
    "        # Its only purpose is to set the model to training mode.\n",
    "        # Some models use mechanisms like Dropout which have distinct behaviors in training and evaluation phases.\n",
    "        model.train()\n",
    "\n",
    "        # No more manual prediction!\n",
    "        # yhat = a + b * x_tensor\n",
    "        yhat = model(x_train_tensor)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y_train_tensor, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # record loss\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # evaluation section\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
